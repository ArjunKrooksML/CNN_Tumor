{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arjunaa Seshadri - 21BTTCL001\n",
    "Deep Learning Project - CNN for Detecting Brain Tumors\n",
    "Used a Brain Tumors MRI Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [128,128]\n",
    "#Setting the Image Size to 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = 'C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\Computer Vision & DL 6th Sem\\\\DL Lab Exps\\\\CNN Project - 1\\\\train'\n",
    "test_dataset = 'C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\Computer Vision & DL 6th Sem\\\\DL Lab Exps\\\\CNN Project - 1\\\\train'\n",
    "#Path to the training and validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Defining the input and output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_layer = keras.Input(shape=(512, 512, 3), name=\"brainimages\")\n",
    "\n",
    "# First convolutional block\n",
    "x = layers.Conv2D(64, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(input_layer)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Second convolutional block\n",
    "x = layers.Conv2D(128, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Third convolutional block\n",
    "x = layers.Conv2D(256, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "\n",
    "# Global average pooling and output layer\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "output_layer = layers.Dense(2, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create the model\n",
    "BrainTumorModel = keras.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Train and Test Data Splits with ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 252 images belonging to 2 classes.\n",
      "Found 252 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dataset, target_size=(128, 128))\n",
    "                                                    \n",
    "validation_generator = test_datagen.flow_from_directory(test_dataset, target_size=(128, 128))\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compiling the model\n",
    "BrainTumorModel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8/8 [==============================] - 8s 722ms/step - loss: 1.0516 - accuracy: 0.6508 - val_loss: 0.6523 - val_accuracy: 0.6111\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 2s 304ms/step - loss: 0.7057 - accuracy: 0.6706 - val_loss: 0.6581 - val_accuracy: 0.6111\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 2s 307ms/step - loss: 0.5751 - accuracy: 0.7421 - val_loss: 0.6588 - val_accuracy: 0.6111\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 2s 293ms/step - loss: 0.4836 - accuracy: 0.7500 - val_loss: 0.6470 - val_accuracy: 0.6111\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 0.4804 - accuracy: 0.7778 - val_loss: 0.6449 - val_accuracy: 0.6111\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 2s 322ms/step - loss: 0.3976 - accuracy: 0.8492 - val_loss: 0.6818 - val_accuracy: 0.6111\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 2s 291ms/step - loss: 0.3853 - accuracy: 0.8413 - val_loss: 0.6439 - val_accuracy: 0.6111\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 2s 289ms/step - loss: 0.3696 - accuracy: 0.8611 - val_loss: 0.6455 - val_accuracy: 0.6111\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 3s 331ms/step - loss: 0.3127 - accuracy: 0.8651 - val_loss: 0.6500 - val_accuracy: 0.6111\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 3s 320ms/step - loss: 0.3619 - accuracy: 0.8452 - val_loss: 0.7056 - val_accuracy: 0.6111\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 2s 300ms/step - loss: 0.3514 - accuracy: 0.8532 - val_loss: 0.6371 - val_accuracy: 0.6111\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 0.3466 - accuracy: 0.8611 - val_loss: 0.6334 - val_accuracy: 0.6111\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 3s 322ms/step - loss: 0.3303 - accuracy: 0.8611 - val_loss: 0.6238 - val_accuracy: 0.6706\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 2s 305ms/step - loss: 0.3323 - accuracy: 0.8492 - val_loss: 0.6436 - val_accuracy: 0.6190\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 2s 291ms/step - loss: 0.2698 - accuracy: 0.8730 - val_loss: 0.6923 - val_accuracy: 0.6111\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 2s 310ms/step - loss: 0.3158 - accuracy: 0.8611 - val_loss: 0.9445 - val_accuracy: 0.3889\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 0.3355 - accuracy: 0.8730 - val_loss: 0.6065 - val_accuracy: 0.6310\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 2s 297ms/step - loss: 0.3108 - accuracy: 0.8690 - val_loss: 0.6886 - val_accuracy: 0.4762\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 2s 310ms/step - loss: 0.2875 - accuracy: 0.8730 - val_loss: 0.5805 - val_accuracy: 0.6230\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.2458 - accuracy: 0.9167 - val_loss: 0.6140 - val_accuracy: 0.7421\n"
     ]
    }
   ],
   "source": [
    "BTfit = BrainTumorModel.fit( train_generator, epochs=20, verbose=1,  validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the class of each tumor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: yes\n",
      "Tumor is present?: no\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m class_indices \u001b[38;5;241m=\u001b[39m train_generator\u001b[38;5;241m.\u001b[39mclass_indices\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:156\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:168\u001b[0m, in \u001b[0;36mIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:370\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    368\u001b[0m filepaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[1;32m--> 370\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     x \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mimg_to_array(img, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arjun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpil_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_indices = train_generator.class_indices\n",
    "\n",
    "\n",
    "\n",
    "for inputs, labels in train_generator:\n",
    "    batch_size = inputs.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        image_class = labels[i]\n",
    "        class_label = list(class_indices.keys())[list(class_indices.values()).index(np.argmax(image_class))]\n",
    "        print(\"Tumor is present?:\", class_label)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a new image to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 202ms/step\n",
      "[[0.22912744 0.370315  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_path = 'Y253.jpg'\n",
    "img = image.load_img(img_path, target_size=(512, 512))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalize the image\n",
    "\n",
    "# Make prediction\n",
    "prediction = BrainTumorModel.predict(img_array)\n",
    "\n",
    "# Print the predicted class\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
